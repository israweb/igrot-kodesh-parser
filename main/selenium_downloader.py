#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ñ‡Ğ¸ĞºĞ° Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Selenium Ğ´Ğ»Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ±Ğ¾Ñ‚Ğ¾Ğ²
"""

import time
import os
import logging
import re
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
import requests


class SeleniumTextDownloader:
    def __init__(self, base_url, download_dir="downloaded_texts", headless=True):
        """
        Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ñ‡Ğ¸ĞºĞ° Ñ Selenium
        
        Args:
            base_url (str): Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ URL Ğ´Ğ»Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ
            download_dir (str): Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
            headless (bool): Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€ Ğ² headless Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ
        """
        self.base_url = base_url
        self.download_dir = download_dir
        self.headless = headless
        self.driver = None
        
        # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¾Ğ² ĞµÑĞ»Ğ¸ ĞµÑ‘ Ğ½ĞµÑ‚
        os.makedirs('../logs', exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('../logs/selenium_downloader.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ
        os.makedirs(self.download_dir, exist_ok=True)
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ñ€Ğ°Ğ¹Ğ²ĞµÑ€Ğ°
        self._init_driver()
    
    def _init_driver(self):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Chrome WebDriver"""
        try:
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument("--headless")
            
            # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--allow-running-insecure-content")
            chrome_options.add_argument("--disable-extensions")
            
            # Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ User-Agent
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            
            # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.logger.info("Chrome WebDriver ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")
            
        except Exception as e:
            self.logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ WebDriver: {e}")
            self.logger.info("Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ ChromeDriver ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² PATH")
            raise
    
    def get_page_with_selenium(self, url, wait_time=10):
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Selenium
        
        Args:
            url (str): URL ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            wait_time (int): Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
            
        Returns:
            BeautifulSoup: ĞŸĞ°Ñ€ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ¸Ğ»Ğ¸ None
        """
        try:
            self.logger.info(f"×˜×•×¢×Ÿ ×“×£ ×¢× Selenium: {url}")
            self.driver.get(url)
            
            # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            WebDriverWait(self.driver, wait_time).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°ÑƒĞ·Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
            time.sleep(3)
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ HTML
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            self.logger.info(f"×“×£ ×˜×•×¢×Ÿ ×‘×”×¦×œ×—×” ({len(html)} ×ª×•×•×™×)")
            return soup
            
        except TimeoutException:
            self.logger.error(f"×˜×™×™×××•×˜ ×‘×˜×¢×™× ×ª ×”×“×£: {url}")
            return None
        except WebDriverException as e:
            self.logger.error(f"×©×’×™××ª WebDriver ×‘×˜×¢×™× ×ª {url}: {e}")
            return None
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×œ× ×¦×¤×•×™×” ×‘×˜×¢×™× ×ª {url}: {e}")
            return None
    
    def extract_text_content(self, soup, url):
        """
        Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        
        Args:
            soup (BeautifulSoup): ĞŸĞ°Ñ€ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°
            url (str): URL ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            
        Returns:
            str: Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ»Ğ¸ None
        """
        try:
            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
                element.decompose()
            
            # ĞŸĞ¾Ğ¸ÑĞº Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°
            content_selectors = [
                'main', 'article', '.content', '.post', '.entry', 
                '.article-body', '.post-content', '.entry-content',
                '[role="main"]', '.main-content', '#content',
                '.text-content', '.article-text'
            ]
            
            main_content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    main_content = content
                    break
            
            if not main_content:
                main_content = soup.find('body')
            
            if main_content:
                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚
                text = main_content.get_text(separator='\n', strip=True)
                
                # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¾Ñ‚ Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¾Ğ²
                text = re.sub(r'\n\s*\n', '\n\n', text)
                text = re.sub(r'[ \t]+', ' ', text)
                
                if text and len(text.strip()) > 200:
                    self.logger.info(f"×˜×§×¡×˜ × ×œ×§×— ××“×£ {url} (××•×¨×š: {len(text)} ×ª×•×•×™×)")
                    return text.strip()
            
            return None
            
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×‘×–×™×”×•×™ ×˜×§×¡×˜ ××“×£ {url}: {e}")
            return None
    
    def save_content(self, content, url, content_type="page"):
        """
        Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ² Ñ„Ğ°Ğ¹Ğ»
        
        Args:
            content (str): Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ
            url (str): URL Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°
            content_type (str): Ğ¢Ğ¸Ğ¿ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾
            
        Returns:
            bool: True ĞµÑĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾
        """
        try:
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ¼ĞµĞ½Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ°
            parsed_url = urlparse(url)
            safe_name = re.sub(r'[<>:"/\\|?*]', '_', parsed_url.path.replace('/', '_'))
            if not safe_name:
                safe_name = f"{content_type}_{int(time.time())}"
            
            filename = f"{content_type}_{safe_name}.txt"
            file_path = os.path.join(self.download_dir, filename)
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°
            counter = 1
            original_path = file_path
            while os.path.exists(file_path):
                name, ext = os.path.splitext(original_path)
                file_path = f"{name}_{counter}{ext}"
                counter += 1
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"××§×•×¨: {url}\n")
                f.write(f"×ª××¨×™×š ×©×™×¢×•×¨: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"×¡×•×’: {content_type}\n")
                f.write("=" * 80 + "\n\n")
                f.write(content)
            
            self.logger.info(f"×§×•×‘×¥ × ×©××¨: {os.path.basename(file_path)} ({len(content)} ×ª×•×•×™×)")
            return True
            
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×‘×©××™×¨×ª ×”×§×•×‘×¥: {e}")
            return False
    
    def find_and_download_files(self, soup, base_url):
        """
        ĞŸĞ¾Ğ¸ÑĞº Ğ¸ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼
        
        Args:
            soup (BeautifulSoup): ĞŸĞ°Ñ€ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°
            base_url (str): Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ URL
            
        Returns:
            int: ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
        """
        downloaded_count = 0
        text_extensions = ['.txt', '.pdf', '.doc', '.docx', '.rtf']
        
        try:
            # ĞŸĞ¾Ğ¸ÑĞº ÑÑÑ‹Ğ»Ğ¾Ğº Ğ½Ğ° Ñ„Ğ°Ğ¹Ğ»Ñ‹
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(base_url, href)
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°
                if any(full_url.lower().endswith(ext) for ext in text_extensions):
                    try:
                        self.logger.info(f"× ××¦× ×§×™×©×•×¨ ×œ×§×•×‘×¥: {full_url}")
                        
                        # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° ÑĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ»
                        response = requests.get(full_url, timeout=30, headers={
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                        })
                        response.raise_for_status()
                        
                        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°
                        filename = os.path.basename(urlparse(full_url).path)
                        if not filename:
                            filename = f"file_{int(time.time())}{os.path.splitext(full_url)[1]}"
                        
                        file_path = os.path.join(self.download_dir, filename)
                        with open(file_path, 'wb') as f:
                            f.write(response.content)
                        
                        self.logger.info(f"×§×•×‘×¥ × ×©××¨: {filename}")
                        downloaded_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"×©×’×™××” ×‘×˜×¢×™× ×ª ×§×•×‘×¥ {full_url}: {e}")
                
                time.sleep(1)  # Ğ—Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸
        
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×‘×—×™×¤×•×© ×§×‘×¦×™×: {e}")
        
        return downloaded_count
    
    def process_url(self, url, extract_content=True, download_files=True):
        """
        ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ URL
        
        Args:
            url (str): URL Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
            extract_content (bool): Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ
            download_files (bool): Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹
            
        Returns:
            dict: Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        """
        results = {
            'success': False,
            'content_saved': False,
            'files_downloaded': 0,
            'error': None
        }
        
        try:
            # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            soup = self.get_page_with_selenium(url)
            if not soup:
                results['error'] = "×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ××ª ×”×“×£"
                return results
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾
            if extract_content:
                text_content = self.extract_text_content(soup, url)
                if text_content:
                    if self.save_content(text_content, url, "content"):
                        results['content_saved'] = True
            
            # ĞŸĞ¾Ğ¸ÑĞº Ğ¸ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
            if download_files:
                files_count = self.find_and_download_files(soup, url)
                results['files_downloaded'] = files_count
            
            results['success'] = True
            
        except Exception as e:
            results['error'] = str(e)
            self.logger.error(f"×©×’×™××” ×‘×¢×™×‘×•×“ URL {url}: {e}")
        
        return results
    
    def run(self, extract_content=True, download_files=True):
        """
        Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        
        Args:
            extract_content (bool): Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ
            download_files (bool): Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹
        """
        self.logger.info(f"××ª×—×™×œ×™× ×¢×™×‘×•×“: {self.base_url}")
        
        try:
            results = self.process_url(
                self.base_url, 
                extract_content=extract_content, 
                download_files=download_files
            )
            
            if results['success']:
                self.logger.info("âœ… ×¢×™×‘×•×“ ×©×œ×!")
                if results['content_saved']:
                    self.logger.info("ğŸ“„ ×ª×•×›×Ÿ ×˜×§×¡×˜×™ × ×©××¨")
                if results['files_downloaded'] > 0:
                    self.logger.info(f"ğŸ“ ×§×‘×¦×™× × ×©××¨×™×: {results['files_downloaded']}")
                if not results['content_saved'] and results['files_downloaded'] == 0:
                    self.logger.warning("âš ï¸ ×œ× × ××¦× ×ª×•×›×Ÿ ×œ×©××™×¨×”")
            else:
                self.logger.error(f"âŒ ×©×’×™××” ×‘×¢×™×‘×•×“: {results.get('error', '×©×’×™××” ×œ× ×™×“×•×¢×”')}")
        
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×§×¨×™×˜×™×ª: {e}")
        
        finally:
            self.close()
    
    def close(self):
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ°"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("WebDriver × ×¡×’×¨")
            except Exception as e:
                self.logger.error(f"×©×’×™××” ×‘×¡×’×™×¨×ª WebDriver: {e}")


def main():
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    target_url = "https://www.chabad.org/therebbe/article_cdo/aid/4643797/jewish/page.htm"
    
    print("ğŸŒ SELENIUM ×˜×•×¢×Ÿ ×˜×§×¡×˜ ×§×‘×¦×™×")
    print("=" * 50)
    print(f"ğŸ¯ URL: {target_url}")
    print("â³ ×ª×—×™×œ×ª ×’×™×‘×•×™ ×“×¤×“×¤×Ÿ...")
    
    try:
        downloader = SeleniumTextDownloader(target_url, headless=True)
        downloader.run(extract_content=True, download_files=True)
        
        print("âœ… ×ª×”×œ×™×š ×©×œ×!")
        print("ğŸ“‹ ×‘×“×•×§ ××ª ×§×•×‘×¥ ×”×œ×•×’ '../logs/selenium_downloader.log' ×œ×¤×¨×˜×™× ××¤×•×¨×˜×™×")
        print("ğŸ“‚ ×§×‘×¦×™× × ×©××¨×™× ×‘×ª×™×§×™×™×ª 'downloaded_texts'")
        
    except Exception as e:
        print(f"âŒ ×©×’×™××”: {e}")
        print("ğŸ’¡ ×•×“× ×©-ChromeDriver ××•×ª×§×Ÿ ×•×–××™×Ÿ ×‘-PATH")


if __name__ == "__main__":
    main()
