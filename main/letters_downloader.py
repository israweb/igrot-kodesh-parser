#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ñ‡Ğ¸Ğº Ğ²ÑĞµÑ… Ğ¿Ğ¸ÑĞµĞ¼ ĞĞ²Ñ€Ğ°Ñ‚ ĞšĞ¾Ğ´ĞµÑˆ (Ğ¡Ğ²ÑÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¸ÑĞµĞ¼) Ñ chabad.org
"""

import time
import os
import logging
import re
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup


class LettersDownloader:
    def __init__(self, download_dir="igrot_kodesh", headless=True):
        """
        ××ª×—×•×œ ××˜×¢×™×Ÿ ×”××›×ª×‘×™×
        
        Args:
            download_dir (str): ×ª×™×§×™×™×” ×œ×©××™×¨×ª ×”××›×ª×‘×™×
            headless (bool): ×œ×”×¤×¢×™×œ ××ª ×”×“×¤×“×¤×Ÿ ×‘××¦×‘ headless
        """
        self.download_dir = download_dir
        self.headless = headless
        self.driver = None
        self.processed_urls = set()
        self.saved_letters = 0
        
        # ×©×™× ×•×™ ×ª×¦×•×¨×ª ×”×œ×•×’×™× ×¢× force=True ×œ×©×™× ×•×™
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        
        # ×™×¦×™×¨×ª ×ª×™×§×™×™×” ×œ×œ×•×’×™× ×× ×”×™× ×œ× ×§×™×™××ª
        os.makedirs('../logs', exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('../logs/letters_downloader.log', encoding='utf-8'),
                logging.StreamHandler()
            ],
            force=True
        )
        self.logger = logging.getLogger(__name__)
        
        # ×™×¦×™×¨×ª ×ª×™×§×™×™×” ×œ××›×ª×‘×™×
        os.makedirs(self.download_dir, exist_ok=True)
        
        # ××ª×—×•×œ ×“×¨×™×™×‘×¨
        self._init_driver()
    
    def _init_driver(self):
        """××ª×—×•×œ ×“×¨×™×™×‘×¨ Chrome WebDriver"""
        try:
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument("--headless")
            
            # ×”×’×“×¨×•×ª ×œ×”×¤×¨×¢×ª ×’×™×‘×•×™ ×“×¤×“×¤×Ÿ
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--allow-running-insecure-content")
            chrome_options.add_argument("--disable-extensions")
            
            # User-Agent ×××©×§×œ×™ ×××©×§×œ
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            
            # ××—×™×§×ª ×¡××œ ×”××•×˜×•××¦×™×”
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.logger.info("Chrome WebDriver × ×˜×¢×Ÿ ×‘×”×¦×œ×—×”")
            
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×‘××ª×—×•×œ ×“×¨×™×™×‘×¨ WebDriver: {e}")
            raise
    
    def get_page_with_selenium(self, url, wait_time=10):
        """×§×‘×œ×ª ×“×£ ×¢× ×¢×–×¨×ª Selenium"""
        try:
            self.logger.info(f"×˜×•×¢×Ÿ ×“×£: {url}")
            self.driver.get(url)
            
            # ×”××ª×Ÿ ×œ×˜×¢×™× ×ª ×”×“×£
            WebDriverWait(self.driver, wait_time).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ×”×× ×”×œ×ª ×ª×¤×•×¡×” × ×•×¡×¤×ª ×œ×˜×¢×™× ×” ××œ××”
            time.sleep(2)
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            self.logger.info(f"×“×£ × ×˜×¢×Ÿ ({len(html)} ×ª×•×•×™×)")
            return soup
            
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×‘×˜×¢×™× ×ª {url}: {e}")
            return None
    
    def find_volume_links(self, soup, base_url):
        """
        ×—×™×¤×•×© ×§×™×©×•×¨×™× ×œ×›×¨×›×™× (×›×¨×›×™×)
        
        Args:
            soup (BeautifulSoup): ×“×£ × ×˜×¢×Ÿ ××—×“×©
            base_url (str): URL ×‘×¡×™×¡×™
            
        Returns:
            list: ×¨×©×™××ª URL ×©×œ ×”×›×¨×›×™×
        """
        volume_links = []
        
        try:
            # ×—×™×¤×•×© ×›×œ ×”×§×™×©×•×¨×™× ×©××›×™×œ×™× "××’×¨×•×ª ×§×•×“×© - ×›×¨×š"
            for link in soup.find_all('a', href=True):
                link_text = link.get_text(strip=True)
                if '××’×¨×•×ª ×§×•×“×© - ×›×¨×š' in link_text and link_text not in ['××’×¨×•×ª ×§×•×“×© Â»']:
                    href = link['href']
                    full_url = urljoin(base_url, href)
                    volume_links.append({
                        'url': full_url,
                        'title': link_text,
                        'volume': link_text
                    })
                    self.logger.info(f"×›×¨×š × ××¦×: {link_text} -> {full_url}")
            
            self.logger.info(f"×›×¨×›×™× × ××¦××•: {len(volume_links)}")
            return volume_links
            
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×‘×—×™×¤×•×© ×›×¨×›×™×: {e}")
            return []
    
    def find_letter_links(self, soup, base_url, volume_title):
        """
        ×—×™×¤×•×© ×§×™×©×•×¨×™× ×œ××›×ª×‘×™× ×‘×•×“×“×™× ×¢× ×ª××™×›×” ×‘×¤×™×’×™× ×¦×™×”
        
        Args:
            soup (BeautifulSoup): ×“×£ × ×˜×¢×Ÿ ×©×œ ×›×¨×š
            base_url (str): URL ×‘×¡×™×¡×™
            volume_title (str): ×›×•×ª×¨×ª ×”×›×¨×š
            
        Returns:
            list: ×¨×©×™××ª URL ×©×œ ×”××›×ª×‘×™×
        """
        all_letters = []
        
        try:
            self.logger.info(f"ğŸ” ××ª×—×™×œ×™× ×—×™×¤×•×© ××›×ª×‘×™× ×‘×›×¨×š {volume_title}")
            self.logger.info(f"ğŸŒ URL ×›×¨×š: {base_url}")
            
            # ×˜×™×¤×•×œ ×‘×›×œ ×“×£ ×©×œ ×”×›×¨×š (×¤×™×’×™× ×¦×™×”)
            current_page = 1
            max_pages = 50  # ×”×’×‘×œ×” ×œ×˜×•×‘×ª ×‘×˜×™×—×•×ª
            
            while current_page <= max_pages:
                if current_page == 1:
                    page_url = base_url
                    page_soup = soup
                else:
                    page_url = f"{base_url}/page/{current_page}"
                    self.logger.info(f"ğŸ“„ ×˜×•×¢×Ÿ ×“×£ {current_page}: {page_url}")
                    page_soup = self.get_page_with_selenium(page_url)
                    if not page_soup:
                        self.logger.info(f"âŒ ×“×£ {current_page} ×œ× × ××¦×, ×¡×™×™×× ×• ××ª ×”×—×™×¤×•×© ×”××›×ª×‘×™×")
                        break
                
                # ×—×™×¤×•×© ××›×ª×‘×™× ×‘×“×£ ×”× ×•×›×—×™
                page_letters = self._extract_letters_from_page(page_soup, page_url, volume_title, current_page)
                
                if page_letters:
                    all_letters.extend(page_letters)
                    self.logger.info(f"ğŸ“ ××›×ª×‘×™× × ××¦××• ×‘×“×£ {current_page}: {len(page_letters)}")
                else:
                    self.logger.info(f"ğŸ“ ×‘×“×£ {current_page} ××›×ª×‘×™× ×œ× × ××¦××•")
                
                # ×‘×“×™×§×ª ×§×™×©×•×¨ ×œ×“×£ ×”×‘×
                next_page_link = page_soup.find('a', {'id': 'Paginator_NextPage'})
                if not next_page_link:
                    # ×—×™×¤×•×© ××œ×˜×¨× ×˜×™×‘×™ ×œ×§×™×©×•×¨ ×œ×“×£ ×”×‘×
                    next_link = page_soup.find('link', {'rel': 'next'})
                    if not next_link:
                        self.logger.info(f"ğŸ“„ ×”×’×¢× ×• ×œ×“×£ ×”××—×¨×•×Ÿ: {current_page}")
                        break
                
                current_page += 1
                time.sleep(1)  # ×”×× ×”×œ×ª ×ª×¤×•×¡×” ×‘×™×Ÿ ×“×¤×™×
            
            # ××—×™×§×ª ×”×›×¤×™×œ×•×™×•×ª
            unique_letters = []
            seen_urls = set()
            for letter in all_letters:
                if letter['url'] not in seen_urls:
                    seen_urls.add(letter['url'])
                    unique_letters.append(letter)
            
            self.logger.info(f"ğŸ“Š ×¡×”\"×› ×‘×›×¨×š {volume_title}:")
            self.logger.info(f"   ğŸ“„ ×“×¤×™× ×©× ×˜×¢× ×•: {current_page}")
            self.logger.info(f"   ğŸ“ ××›×ª×‘×™× × ××¦××•: {len(unique_letters)}")
            
            # ×¨×™×©×•× ×“×•×’×××•×ª ×©×œ ×”××›×ª×‘×™× ×”×¨××©×•× ×™×
            if unique_letters:
                self.logger.info(f"ğŸ“‹ ×“×•×’×××•×ª ×œ××›×ª×‘×™× × ××¦××•:")
                for i, letter in enumerate(unique_letters[:5], 1):
                    self.logger.info(f"   {i}. {letter['title']}")
            
            return unique_letters
            
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘×—×™×¤×•×© ××›×ª×‘×™× ×‘×›×¨×š {volume_title}: {e}")
            return []
    
    def _extract_letters_from_page(self, soup, page_url, volume_title, page_num):
        """
        ×”×•×¦××ª ××›×ª×‘×™× ××“×£ ××—×“
        
        Args:
            soup (BeautifulSoup): ×“×£ × ×˜×¢×Ÿ
            page_url (str): URL ×©×œ ×”×“×£
            volume_title (str): ×›×•×ª×¨×ª ×”×›×¨×š
            page_num (int): ××¡×¤×¨ ×”×“×£
            
        Returns:
            list: ×¨×©×™××ª ×”××›×ª×‘×™× ×‘×“×£
        """
        letters = []
        
        try:
            # ×—×™×¤×•×© ××œ×× ×˜×™× ×¢× ×˜×§×¡×˜ "××›×ª×‘" (××›×ª×‘)
            letter_elements = soup.find_all(text=re.compile(r'××›×ª×‘'))
            self.logger.info(f"ğŸ”¤ ×‘×“×£ {page_num} × ××¦××• ××œ×× ×˜×™× ×¢× '××›×ª×‘': {len(letter_elements)}")
            
            for element in letter_elements:
                # ××¦×™××ª ×”××œ×× ×˜ ×”×”×•×¨×” ×¢× ×§×™×©×•×¨
                parent = element.parent
                while parent and parent.name != 'html':
                    link = parent.find('a', href=True)
                    if link:
                        href = link.get('href', '')
                        text = element.strip()
                        
                        # ×‘×“×™×§×ª ×§×™×©×•×¨ ×œ××›×ª×‘
                        if href and 'letter' in href.lower() or 'aid=' in href:
                            full_url = urljoin(page_url, href)
                            letters.append({
                                'url': full_url,
                                'title': text,
                                'volume': volume_title,
                                'page': page_num,
                                'href': href
                            })
                            break
                    parent = parent.parent
            
            # ×—×™×¤×•×© ××œ×˜×¨× ×˜×™×‘×™: ×—×™×¤×•×© ×§×™×©×•×¨×™× ×¢× ×ª×‘× ×™×•×ª ××¡×•×™××•×ª ×‘×˜×§×¡×˜
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # ×‘×“×™×§×ª ×ª×‘× ×™×•×ª ×œ××›×ª×‘×™×
                if (text and 
                    ('××›×ª×‘' in text or 
                     re.search(r'\d+', text) and len(text) < 20 and
                     any(char in '××‘×’×“×”×•×–×—×˜×™×›×œ×× ×¡×¢×¤×¦×§×¨×©×ª' for char in text))):
                    
                    # ×¡×™×œ×•×§ ×§×™×©×•×¨×™× × ×™×•×•×˜×™×™×
                    if not any(pattern in text.lower() for pattern in 
                              ['browse', 'next', 'previous', 'page', 'home']):
                        
                        full_url = urljoin(page_url, href)
                        # ×¡×™×œ×•×§ ×›×¤×™×œ×•×™×•×ª
                        if not any(l['url'] == full_url for l in letters):
                            letters.append({
                                'url': full_url,
                                'title': text,
                                'volume': volume_title,
                                'page': page_num,
                                'href': href
                            })
            
            return letters
            
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘×”×•×¦××ª ××›×ª×‘×™× ××“×£ {page_num}: {e}")
            return []
    
    def extract_letter_content(self, soup, url):
        """×”×•×¦××ª ×ª×•×›×Ÿ ×”××›×ª×‘"""
        try:
            # ×¡×™×œ×•×§ ×˜×™×¤×•×¡×™× ×œ× ×¨×¦×•×™×™×
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
                element.decompose()
            
            # ×—×™×¤×•×© ×ª×•×›×Ÿ ×”××›×ª×‘ ×”×¢×™×§×¨×™
            content_selectors = [
                '.article-body', '.post-content', '.entry-content',
                '.content', '.text-content', '.letter-content',
                'main', 'article', '[role="main"]'
            ]
            
            main_content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    main_content = content
                    break
            
            if not main_content:
                # ×× ×œ× ××¦×× ×• ×˜×™×¤×•×¡ ×¡×¤×¦×™×¤×™, ×—×™×¤×•×© ×“×™×‘ ×¢× ×”×¨×‘×” ×˜×§×¡×˜
                divs = soup.find_all('div')
                for div in divs:
                    text = div.get_text(strip=True)
                    if len(text) > 500:  # ××™× ×™××•× 500 ×ª×•×•×™×
                        main_content = div
                        break
            
            if not main_content:
                main_content = soup.find('body')
            
            if main_content:
                # ×”×•×¦××ª ×˜×§×¡×˜
                text = main_content.get_text(separator='\n', strip=True)
                
                # × ×™×§×•×™ ××¨×•×•×—×™× ×•×©×•×¨×•×ª ×—×“×©×•×ª
                text = re.sub(r'\n\s*\n', '\n\n', text)
                text = re.sub(r'[ \t]+', ' ', text)
                
                if text and len(text.strip()) > 100:
                    return text.strip()
            
            return None
            
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×‘×”×•×¦××ª ×ª×•×›×Ÿ ×”××›×ª×‘ {url}: {e}")
            return None
    
    def save_letter(self, content, letter_info):
        """×©××™×¨×ª ××›×ª×‘ ×‘×§×•×‘×¥"""
        try:
            # ×”×•×¦××ª ×›×¨×š ×•××¡×¤×¨ ×”××›×ª×‘
            volume_title = letter_info['volume']  # ×œ××©×œ: "××’×¨×•×ª ×§×•×“×© - ×›×¨×š ×"
            original_title = letter_info['title']  # ×œ××©×œ: "××’×¨×•×ª ×§×•×“×© - ××›×ª×‘ ×¤×“"
            
            # ×”×•×¦××ª ×›×¨×š (×›×¨×š X)
            volume_match = re.search(r'(×›×¨×š\s+[×-×ª]+)', volume_title)
            volume_part = volume_match.group(1) if volume_match else "×›×¨×š ×"
            
            # ×”×•×¦××ª ××¡×¤×¨ ×”××›×ª×‘ (××›×ª×‘ X)
            letter_match = re.search(r'(××›×ª×‘\s+[×-×ª]+)', original_title)
            letter_part = letter_match.group(1) if letter_match else "××›×ª×‘ ×"
            
            # ×™×¦×™×¨×ª ×›×•×ª×¨×ª ×œ×ª×•×›×Ÿ ×”×§×•×‘×¥
            # ×¤×•×¨××˜: ××’×¨×•×ª ×§×•×“×© - ×›×¨×š × - ××›×ª×‘ ×¤×“
            file_header = f"××’×¨×•×ª ×§×•×“×© - {volume_part} - {letter_part}"
            
            # ×™×¦×™×¨×ª ×©× ×§×•×‘×¥ ×¢× ×©××•×ª ×§×¦×¨×™×
            # ×¤×•×¨××˜: ××§ - ×›×¨×š × - ××›×ª×‘ ×¤×“
            filename_base = f"××§ - {volume_part} - {letter_part}"
            
            # × ×™×§×•×™ ×©××•×ª ×§×‘×¦×™× ××ª×•×•×›×™× (××©××™×¨ ×¨×§ ××•×ª×™×•×ª ×•×¡×™×× ×™× ×‘×¡×™×¡×™×™×)
            safe_filename = re.sub(r'[<>:"/\\|?*]', '_', filename_base)
            safe_filename = re.sub(r'[^\w\s\-_\u0590-\u05FF]', '', safe_filename)
            
            filename = f"{safe_filename}.txt"
            if len(filename) > 200:  # ×”×’×‘×œ×ª ××•×¨×š ×©× ×”×§×•×‘×¥
                filename = filename[:190] + ".txt"
            
            file_path = os.path.join(self.download_dir, filename)
            
            # ×‘×“×™×§×ª ×§×™×•× ×§×•×‘×¥
            counter = 1
            original_path = file_path
            while os.path.exists(file_path):
                name, ext = os.path.splitext(original_path)
                file_path = f"{name}_{counter}{ext}"
                counter += 1
            
            # ×©××™×¨×ª ×”×§×•×‘×¥ ×¢× ×¤×•×¨××˜ ×›×•×ª×¨×ª ×—×“×©
            with open(file_path, 'w', encoding='utf-8') as f:
                # ×¨×§ ×›×•×ª×¨×ª ×”××›×ª×‘ ×‘×¤×•×¨××˜ ×”×—×“×©
                f.write(f"{file_header}\n\n")
                f.write(content)
            
            self.logger.info(f"××›×ª×‘ × ×©××¨: {os.path.basename(file_path)} ({len(content)} ×ª×•×•×™×)")
            self.saved_letters += 1
            return True
            
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×‘×©××™×¨×ª ×”××›×ª×‘: {e}")
            return False
    
    def download_letters_from_volume(self, volume_info):
        """×”×•×¨×“×ª ×›×œ ×”××›×ª×‘×™× ××›×¨×š ××—×“"""
        self.logger.info(f"××ª×—×™×œ×™× ×˜×™×¤×•×œ ×‘×›×¨×š: {volume_info['title']}")
        
        # ×§×‘×œ×ª ×“×£ ×”×›×¨×š
        soup = self.get_page_with_selenium(volume_info['url'])
        if not soup:
            return 0
        
        # ×—×™×¤×•×© ×§×™×©×•×¨×™× ×œ××›×ª×‘×™×
        letter_links = self.find_letter_links(soup, volume_info['url'], volume_info['title'])
        
        downloaded_count = 0
        for i, letter_info in enumerate(letter_links, 1):
            if letter_info['url'] in self.processed_urls:
                continue
                
            self.processed_urls.add(letter_info['url'])
            
            self.logger.info(f"×˜×™×¤×•×œ ×‘××›×ª×‘ {i}/{len(letter_links)}: {letter_info['title']}")
            
            # ×§×‘×œ×ª ×ª×•×›×Ÿ ×”××›×ª×‘
            letter_soup = self.get_page_with_selenium(letter_info['url'])
            if letter_soup:
                content = self.extract_letter_content(letter_soup, letter_info['url'])
                if content:
                    if self.save_letter(content, letter_info):
                        downloaded_count += 1
                else:
                    self.logger.warning(f"×œ× × ×™×ª×Ÿ ×œ×”×•×¦×™× ×ª×•×›×Ÿ: {letter_info['title']}")
            
            # ×”×× ×”×œ×ª ×ª×¤×•×¡×” ×‘×™×Ÿ ××›×ª×‘×™×
            time.sleep(2)
        
        self.logger.info(f"××›×ª×‘×™× × ×©××¨×• ××›×¨×š {volume_info['title']}: {downloaded_count}")
        return downloaded_count
    
    def download_all_letters(self, start_url):
        """×©×™×˜×” ×¨××©×™×ª ×œ×”×•×¨×“×ª ×›×œ ×”××›×ª×‘×™×"""
        self.logger.info("ğŸš€ ××ª×—×™×œ×™× ×”×•×¨×“×ª ×›×œ ×”××›×ª×‘×™× ×©×œ ××‘×¨×˜ ×§×•×“×©")
        self.logger.info(f"×“×£ ××¤×ª×—: {start_url}")
        
        try:
            # ×§×‘×œ×ª ×“×£ ×”×¨××©×™ ×¢× ×¨×©×™××ª ×”×›×¨×›×™×
            soup = self.get_page_with_selenium(start_url)
            if not soup:
                self.logger.error("×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ××ª ×“×£ ×”×¨××©×™")
                return
            
            # ×—×™×¤×•×© ×›×œ ×”×›×¨×›×™×
            volume_links = self.find_volume_links(soup, start_url)
            if not volume_links:
                self.logger.error("×œ× × ××¦××• ×§×™×©×•×¨×™× ×œ×›×¨×›×™×")
                return
            
            total_letters = 0
            
            # ×˜×™×¤×•×œ ×‘×›×œ ×›×¨×š
            for i, volume_info in enumerate(volume_links, 1):
                self.logger.info(f"ğŸ“š ×˜×™×¤×•×œ ×‘×›×¨×š {i}/{len(volume_links)}: {volume_info['title']}")
                
                letters_count = self.download_letters_from_volume(volume_info)
                total_letters += letters_count
                
                # ×”×× ×”×œ×ª ×ª×¤×•×¡×” ×‘×™×Ÿ ×›×¨×›×™×
                if i < len(volume_links):
                    self.logger.info("â³ ×”×× ×”×œ×ª ×ª×¤×•×¡×” 5 ×©× ×™×•×ª ×‘×™×Ÿ ×›×¨×›×™×...")
                    time.sleep(5)
            
            self.logger.info(f"ğŸ‰ ×¡×™×™×× ×•! ×¡×”\"×› × ×©××¨×• ××›×ª×‘×™×: {total_letters}")
            self.logger.info(f"ğŸ“‚ ×”××›×ª×‘×™× × ×©××¨×• ×‘×ª×™×§×™×™×”: {self.download_dir}")
            
        except Exception as e:
            self.logger.error(f"×©×’×™××” ×§×¨×™×˜×™×ª: {e}")
        finally:
            self.close()
    
    def close(self):
        """×¡×’×™×¨×ª ×”×“×¤×“×¤×Ÿ"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("WebDriver × ×¡×’×¨")
            except Exception as e:
                self.logger.error(f"×©×’×™××” ×‘×¡×’×™×¨×ª WebDriver: {e}")


def main():
    """×©×™×˜×” ×¨××©×™×ª"""
    start_url = "https://www.chabad.org/therebbe/article_cdo/aid/4643797/jewish/page.htm"
    
    print("ğŸ“š ××˜×¢×™×Ÿ ××›×ª×‘×™ ××’×¨×•×ª ×§×•×“×©")
    print("=" * 50)
    print("ğŸ¯ ××ª×—×™×œ×™× ×”×•×¨×“×ª ×›×œ ×”××›×ª×‘×™× ××›×œ ×”×›×¨×›×™×")
    print("ğŸ“‚ ×”×ª×•×¦××” ×ª×•×©××¨ ×‘×ª×™×§×™×™×” 'igrot_kodesh'")
    print("â° ×–×” ×™×›×•×œ ×œ×§×—×ª ×›××” ×©×¢×•×ª...")
    print("=" * 50)
    
    try:
        downloader = LettersDownloader(download_dir="igrot_kodesh", headless=True)
        downloader.download_all_letters(start_url)
        
        print("\nâœ… ×”×ª×”×œ×™×š ×¡×™×™×!")
        print("ğŸ“‹ ×‘×“×•×§ ××ª ×§×•×‘×¥ ×”×œ×•×’ '../logs/letters_downloader.log' ×œ×¤×¨×˜×™×")
        print("ğŸ“‚ ×”××›×ª×‘×™× × ××¦××™× ×‘×ª×™×§×™×™×” 'igrot_kodesh'")
        
    except Exception as e:
        print(f"âŒ ×©×’×™××”: {e}")


if __name__ == "__main__":
    main()
